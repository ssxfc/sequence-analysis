# tech-ml
机器学习相关技术

# 常识
* 模型本身有一个隐含属性，也就是其学习能力。学习能力可以简单理解为模型复杂性，当学习能力强的情况下
经过多轮学习，一般可以降低损失函数值。因为学习能力强，可以比较好的学习到目标的本质或接近目标本质。
* 导致训练损失降不下来的可能原因
  * 模型复杂度不够，或学习能力差。
  * 数据量少。也就意味着模型即使有很强的学习能力，也很难从少量信息中有效学习到目标本质
  * 数据质量差。容易导致模型学习误入歧途。
  * 超参数设置不合理。如学习率设置过大导致跳过最优解，再如学习轮次不够，学习不充分。
  * 梯度消失或爆炸。深层模型的浅层参数无法更新或浅层参数抖动
  * 优化器对损失函数的影响。如SGD可能在某些非凸优化问题中容易陷入局部最优解

# 非凸优化问题
## 凸集和凸函数
**凸集：** 在数学中，凸集是指对于集合中的任意两点，连接这两点的线段上的所有点都在该集合内。
**凸函数：** 定义在凸集上的函数，凸函数的形状像一个 “碗”，没有局部最小值（除了全局最小值）

## 非凸优化问题
简单来讲就是在非凸函数上找全局最优解（全局最小值）
## 应对非凸优化问题的策略
* **探索性优化器**。即使到达局部最优解，也能进行探索并跳出局部最优解。
* **模型初始化和正则化**。模型初始化能帮助网络从一个比较合适的位置开始训练。而正则化一方面能减缓过拟合，另一方面能改变了损失函数的形状，有利于跳出局部最优解，如Dropout正则化随机失活部分神经元，创造了跳出局部最优解的有利条件。
* **模型融合** 通过融合多个模型进行综合训练。

# 模型参数初始化
## 零初始化
## 